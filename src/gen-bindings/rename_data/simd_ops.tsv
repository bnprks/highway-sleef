# Source call -> translation
# Note: this TSV allows comments and empty lines

############
# Arithmetic
############
vadd_vf_vf_vf(x, y)	Add(x, y)
vadd_vi2_vi2_vi2(x, y)	Add(x, y)
vadd_vd_vd_vd(x, y)	Add(x, y)
vadd_vi_vi_vi(x, y)	Add(x, y)
vadd64_vm_vm_vm(x, y)	Add(x, y)

vsub_vi2_vi2_vi2(x, y)	Sub(x, y)
vsub_vf_vf_vf(x, y)	Sub(x, y)
vsub_vi_vi_vi(x, y)	Sub(x, y)
vsub_vd_vd_vd(x, y)	Sub(x, y)
vsub64_vm_vm_vm(x, y)	Sub(x, y)

vfma_vf_vf_vf_vf(x, y, z)	MulAdd(x, y, z)
vmla_vf_vf_vf_vf(x, y, z)	MulAdd(x, y, z)
vfmapn_vf_vf_vf_vf(x, y, z)	MulSub(x, y, z)
vmlapn_vf_vf_vf_vf(x, y, z)	MulSub(x, y, z)
vfmanp_vf_vf_vf_vf(x, y, z)	NegMulAdd(x, y, z)
vmlanp_vf_vf_vf_vf(x, y, z)	NegMulAdd(x, y, z)
vfma_vd_vd_vd_vd(x, y, z)	MulAdd(x, y, z)
vmla_vd_vd_vd_vd(x, y, z)	MulAdd(x, y, z)
vfmapn_vd_vd_vd_vd(x, y, z)	MulSub(x, y, z)
vmlapn_vd_vd_vd_vd(x, y, z)	MulSub(x, y, z)
vfmanp_vd_vd_vd_vd(x, y, z)	NegMulAdd(x, y, z)
vmlanp_vd_vd_vd_vd(x, y, z)	NegMulAdd(x, y, z)

vmul_vf_vf_vf(x, y)	Mul(x, y)
vrec_vf_vf(x)	Div(Set(df, 1.0), x)
vdiv_vf_vf_vf(x, y)	Div(x, y)
vsqrt_vf_vf(x)	Sqrt(x)
vmul_vd_vd_vd(x, y)	Mul(x, y)
vrec_vd_vd(x)	Div(Set(df, 1.0), x)
vdiv_vd_vd_vd(x, y)	Div(x, y)
vsqrt_vd_vd(x)	Sqrt(x)

visinf_vo_vf(x)	IsInf(x)
visnan_vo_vf(x)	IsNaN(x)
vispinf_vo_vf(x)	Eq(x, Inf(df))
visinf_vo_vd(x)	IsInf(x)
visnan_vo_vd(x)	IsNaN(x)
vispinf_vo_vd(x)	Eq(x, Inf(df))

vabs_vf_vf(x)	Abs(x)
vneg_vi2_vi2(x)	Neg(x)
vneg_vf_vf(x)	Neg(x)
vabs_vd_vd(x)	Abs(x)
vneg_vi_vi(x)	Neg(x)
vneg_vd_vd(x)	Neg(x)

vmax_vf_vf_vf(x, y)	Max(x, y)
vmin_vf_vf_vf(x, y)	Min(x, y)
vmax_vd_vd_vd(x, y)	Max(x, y)
vmin_vd_vd_vd(x, y)	Min(x, y)

vrint_vi2_vf(f)	NearestInt(f) # NOTE: Slower than Sleef due to FixConversionOverflow call
vrint_vi_vd(f)	ConvertTo(di, Round(f)) # NOTE: Slower than Sleef due to FixConversionOverflow call
vrint_vf_vf(f)	Round(f)
vrint_vd_vd(f)	Round(f)
vrint2_vd_vd(f)	Round(f) # This is the definition if FULL_FP_ROUNDING is defined (which seems to be most architectures)
vtruncate_vf_vf(f)	Trunc(f)
vtruncate_vd_vd(f)	Trunc(f)
vtruncate_vi_vd(f)	ConvertTo(di, Trunc(f))

# These are used only with AVX512
vgetexp_vf_vf(f)	GetExponent(f)
vgetmant_vf_vf(f)	GetMantissa(f)
vfixup_vf_vf_vf_vi2_i(a, b, c, imm)	Fixup<imm>(a, b, c)
vgetexp_vd_vd(f)	GetExponent(f)
vgetmant_vd_vd(f)	GetMantissa(f)
vfixup_vd_vd_vd_vi2_i(a, b, c, imm)	Fixup<imm>(a, b, c)

# These are used only with 32-bit ARM NEON
vmlaq_f32(a, b, c)	MulAdd(b, c, a)
vrsqrteq_f32(a)	Vec<D>{vrsqrteq_f32(a.raw)}
vrsqrtsq_f32(a, b)	Vec<D>{vrsqrtsq_f32(a.raw, b.raw)}
vmlsq_f32(a, b, c)	NegMulAdd(b, c, a)
vdupq_n_f32(a)	Set(df, a)
vmulq_f32(a, b)	Mul(a, b)
vfmsq_f32(a, b, c)	NegMulAdd(b, c, a) # These orderings are tricky
vfmaq_f32(a, b, c)	MulAdd(b, c, a) # These orderings are tricky

############
# Bitwise
############
vsll_vi2_vi2_i(x, c)	ShiftLeft<c>(x)
vsra_vi2_vi2_i(x, c)	ShiftRight<c>(x)
vsrl_vi2_vi2_i(x, c)	BitCast(di, ShiftRight<c>(BitCast(du, x))) # Convert to unsigned to get a logical shift
vsll_vi_vi_i(x, c)	ShiftLeft<c>(x)
vsra_vi_vi_i(x, c)	ShiftRight<c>(x)
vsrl_vi_vi_i(x, c)	BitCast(di, ShiftRight<c>(BitCast(du, x))) # Convert to unsigned to get a logical shift
vsrl64_vm_vm_i(x, c)	ShiftRight<c>(x)

vand_vi2_vi2_vi2(x, y)	And(x, y)
vand_vi_vi_vi(x, y)	And(x, y)
vand_vm_vm_vm(x, y)	And(x, y)

vandnot_vi2_vi2_vi2(x, y)	AndNot(x, y)
vandnot_vi_vi_vi(x, y)	AndNot(x, y)

vor_vi2_vi2_vi2(x, y)	Or(x, y)
vor_vm_vm_vm(x, y)	Or(x, y)
vxor_vm_vm_vm(x, y)	Xor(x, y)
vgt_vi2_vi2_vi2(x, y)	VecFromMask(di, Gt(x, y))

############
# Masks
############
# Note: All masks should be returned as type df, and assumed to come as type df

veq_vo_vi2_vi2(x, y)	RebindMask(df, Eq(x, y))
veq_vo_vf_vf(x, y)	Eq(x, y)
vge_vo_vf_vf(x, y)	Ge(x, y)
vgt_vo_vf_vf(x, y)	Gt(x, y)
vgt_vo_vi2_vi2(x, y)	RebindMask(df, Gt(x, y))
visnegzero_vo_vf(x)	Eq(x, Set(df, -0.0))
vlt_vo_vf_vf(x, y)	Lt(x, y)
veq_vo_vi_vi(x, y)	RebindMask(df, Eq(x, y))
veq_vo_vd_vd(x, y)	Eq(x, y)
vge_vo_vd_vd(x, y)	Ge(x, y)
vgt_vo_vd_vd(x, y)	Gt(x, y)
vgt_vo_vi_vi(x, y)	RebindMask(df, Gt(x, y))
visnegzero_vo_vd(x)	Eq(x, Set(df, -0.0))
vlt_vo_vd_vd(x, y)	Lt(x, y)
vneq_vo_vd_vd(x, y)	Ne(x, y)
veq64_vo_vm_vm(x, y) 	RebindMask(df, Eq(x, y))

vand_vo_vo_vo(x, y)	And(x, y)
vor_vo_vo_vo(x, y)	Or(x, y)
vxor_vo_vo_vo(x, y)	Xor(x, y)
vandnot_vo_vo_vo(x, y)	AndNot(x, y)

vsel_vf_vo_vf_vf(m, x, y)	IfThenElse(m, x, y)
vsel_vi2_vo_vi2_vi2(m, x, y)	IfThenElse(RebindMask(di, m), x, y)
vsel_vi2_vf_vf_vi2_vi2(f0, f1, x, y)	IfThenElse(RebindMask(di, Lt(f0, f1)), x, y)
vsel_vd_vo_vd_vd(m, x, y)	IfThenElse(m, x, y)
vsel_vi_vo_vi_vi(m, x, y)	IfThenElse(RebindMask(di, m), x, y)

vtestallones_i_vo32(m)	AllTrue(df, m)
vtestallones_i_vo64(m)	AllTrue(df, m)

# Combining unsigned int masks with true masks is a bit tricky
vor_vm_vo32_vm(m, x)	IfThenElse(RebindMask(du, m), Set(du, -1), x)
vandnot_vm_vo32_vm(m, y)	IfThenZeroElse(RebindMask(du, m), y)
vor_vm_vo64_vm(m, x)	IfThenElse(RebindMask(du, m), Set(du, -1), x)
vandnot_vm_vo64_vm(m, y)	IfThenZeroElse(RebindMask(du, m), y)
vand_vm_vo32_vm(m, y)	IfThenElseZero(RebindMask(du, m), y)
vand_vm_vo64_vm(m, y)	IfThenElseZero(RebindMask(du, m), y)
vand_vi2_vo_vi2(m, y)	IfThenElseZero(RebindMask(di, m), y)
vand_vi_vo_vi(m, y)	IfThenElseZero(m, y)

############
# Conversions
############
vcast_vf_f(f)	Set(df, f)
vcast_vd_d(f)	Set(df, f)
vcast_vi2_i(i)	Set(di, i)
vcast_vi_i(i)	Set(di, i)
vcastu_vm_vi(i)	ShiftLeft<32>(BitCast(du, i))
vcastu_vi_vm(i)	BitCast(di, ShiftRight<32>(i))
vcast_vf_vi2(vi)	ConvertTo(df, vi)
vcast_vd_vi(vi)	ConvertTo(df, vi)
vtruncate_vi2_vf(f)	ConvertTo(di, f)
vcast_vm_i_i(i0, i1)	Set(du, (static_cast<uint64_t>(i0) << 32) | i1)
vcast_vo32_vo64(i)	i # I think these become no-ops when sticking to 64-bit ints with double precision
vcast_vo64_vo32(i)	i # I think these become no-ops when sticking to 64-bit ints with double precision

vreinterpret_vf_vi2(i)	BitCast(df, i)
vreinterpret_vf_vm(m)	BitCast(df, m)
vreinterpret_vd_vm(m)	BitCast(df, m)
vreinterpret_vi2_vf(f)	BitCast(di, f)
vreinterpret_vm_vf(m)	BitCast(du, m)
vreinterpret_vm_vd(m)	BitCast(du, m)

############
# 2-float Tuples
############
vf2getx_vf_vf2(f)	Get2<0>(f)
vf2gety_vf_vf2(f)	Get2<1>(f)
vf2setx_vf2_vf2_vf(f2, f)	Set2<0>(f2, f)
vf2sety_vf2_vf2_vf(f2, f)	Set2<1>(f2, f)
vcast_vf2_vf_vf(x, y)	Create2(df, x, y)
vcast_vf2_f_f(x, y)	Create2(df, Set(df, x), Set(df, y))
vf2setxy_vf2_vf_vf(x, y)	Create2(df, x, y)

vd2getx_vd_vd2(f)	Get2<0>(f)
vd2gety_vd_vd2(f)	Get2<1>(f)
vd2setx_vd2_vd2_vd(f2, f)	Set2<0>(f2, f)
vd2sety_vd2_vd2_vd(f2, f)	Set2<1>(f2, f)
vcast_vd2_vd_vd(x, y)	Create2(df, x, y)
vcast_vd2_d_d(x, y)	Create2(df, Set(df, x), Set(df, y))
vd2setxy_vd2_vd_vd(x, y)	Create2(df, x, y)

############
# (float, int) Tuples
############
figeti_vi2_di(fi)	BitCast(di, Get2<1>(fi))
figetd_vf_di(fi)	Get2<0>(fi)
fisetdi_fi_vf_vi2(f, i)	Create2(df, f, BitCast(df, i))

disetdi_di_vd_vi(f, i)	Create2(df, f, BitCast(df, i))
digetd_vd_di(fi)	Get2<0>(fi)
digeti_vi_di(fi)	BitCast(di, Get2<1>(fi))

############
# 2-float + int Tuples (float, float, int)
############
dfisetdf_dfi_dfi_vf2(f2i, f2)	Set3<0>(Set3<1>(f2i, Get2<1>(f2)), Get2<0>(f2))  # Set the 2-float portion
dfigeti_vi2_dfi(f2i)	BitCast(di, Get3<2>(f2i))                            # Get the int portion
dfigetdf_vf2_dfi(f2i)	Create2(df, Get3<0>(f2i), Get3<1>(f2i))              # Get the 2-float portion
dfisetdfi_dfi_vf2_vi2(f2, i)	Create3(df, Get2<0>(f2), Get2<1>(f2), BitCast(df, i)) # Set the full value

ddisetdd_ddi_ddi_vd2(f2i, f2)	Set3<0>(Set3<1>(f2i, Get2<1>(f2)), Get2<0>(f2))  # Set the 2-float portion
ddigeti_vi_ddi(f2i)	BitCast(di, Get3<2>(f2i))                            # Get the int portion
ddigetdd_vd2_ddi(f2i)	Create2(df, Get3<0>(f2i), Get3<1>(f2i))              # Get the 2-double portion
ddisetddi_ddi_vd2_vi(f2, i)	Create3(df, Get2<0>(f2), Get2<1>(f2), BitCast(df, i)) # Set the full value

############
# Misc
############
LIKELY(x)	HWY_LIKELY(x)
INT64_C(x)	INT64_C(x)
vgather_vf_p_vi2(ptr, idx)	GatherIndex(df, ptr, idx)
vgather_vd_p_vi(ptr, idx)	GatherIndex(df, ptr, idx)

# Sleef's implementation of Estrin's method uses pre-computed x^2 and x^4
POLY6(x, x2, x4, c5, c4, c3, c2, c1, c0)	Estrin(x, x2, x4, Set(df, c0), Set(df, c1), Set(df, c2), Set(df, c3), Set(df, c4), Set(df, c5))
POLY7(x, x2, x4, c6, c5, c4, c3, c2, c1, c0)	Estrin(x, x2, x4, Set(df, c0), Set(df, c1), Set(df, c2), Set(df, c3), Set(df, c4), Set(df, c5), Set(df, c6))
POLY8(x, x2, x4, c7, c6, c5, c4, c3, c2, c1, c0)	Estrin(x, x2, x4, Set(df, c0), Set(df, c1), Set(df, c2), Set(df, c3), Set(df, c4), Set(df, c5), Set(df, c6), Set(df, c7))
POLY9(x, x2, x4, x8, c8, c7, c6, c5, c4, c3, c2, c1, c0)	Estrin(x, x2, x4, x8, Set(df, c0), Set(df, c1), Set(df, c2), Set(df, c3), Set(df, c4), Set(df, c5), Set(df, c6), Set(df, c7), Set(df, c8))
POLY10(x, x2, x4, x8, c9, c8, c7, c6, c5, c4, c3, c2, c1, c0)	Estrin(x, x2, x4, x8, Set(df, c0), Set(df, c1), Set(df, c2), Set(df, c3), Set(df, c4), Set(df, c5), Set(df, c6), Set(df, c7), Set(df, c8), Set(df, c9))